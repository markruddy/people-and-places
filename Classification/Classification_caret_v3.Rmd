---
title: "Survey Group - Classification"
author: "Mark Ruddy"
date: "29 August, 2016"
bibliography: bibliodb.bib
output: html_document
---

<!--
************************************************************************************
INSTRUCTIONS TO RUN R-code

1. Install R (https://cran.rstudio.com/) and  RStudio (https://www.rstudio.com/)
2. Read the user docs
3. Within code 'chunks' change eval=FALSE to eval=TRUE
4. Run R code and reproduce the steps within the study

************************************************************************************
-->

```{r Load_data, include=FALSE, eval=TRUE}
###########################
# IGNORE THIS
# There are Mark Ruddy's Working directory variables
setwd("~/Documents/Personal/Work/DataScience/PeopleAndPlaces/Working/Classification")
# load("./Data/pp.class.base.RData") # Base data for reproducibility
load("./Data/pp.class_caret_v3.RData") # saved dataset with all objects on
###########################
```

<!-- Using: -->
<!-- * Max Kuhn's caret package manual <http://topepo.github.io/caret/index.html> [@kuhn:2016aa] -->
<!-- * Max Kuhn's 'Predictive Modeling with R and the caret Package' presentation  <http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf> [@kuhn:2013aa] -->
<!-- * James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer-Verlag New York. [@james_introduction_2013] -->
<!-- * Hsu, C.-W., Chang, C.-C., and Lin, C.-J. (2016). A Practical Guide to Support Vector Classification. Technical report, Department of Computer Science National Taiwan University, Taipei 106, Taiwan. [@hsu:2016aa] -->
<!-- * Grolemund, G. and Wickham, H. (2016). *R for Data Science*. [@grolemund:2016aa] 

## Useful blogs:

[#ra-svm]: http://blog.revolutionanalytics.com/2015/10/the-5th-tribe-support-vector-machines-and-caret.html
[#go-svm]: https://geekoverdose.wordpress.com/2014/07/25/svm-classification-example-with-performance-measures-using-r-caret/
[#sr-SL-intro]: http://sebastianraschka.com/Articles/2014_intro_supervised_learning.html
-->

## Support Vector Machine (SVM) classification 



### Description

A number of different classification techniques are available, each with their own pros, cons, and assumptions. For this study Support Vector Machines is used. The Support Vector Machine (SVM) is a supervised machine learning method, well-suited to solving classification problems in complex datasets, including non-linear ones. SVMs have a number of other general strengths advantageous to this study [@aggarwal_introduction_2015]:

* Resistance to overfitting (loss of ability to generalise).
* Provide globally optimal 'convex' solution: the same answer for same parameters across multiple model runs.
* No ‘black box’ - *cf*. Artificial Neural Networks (ANNs).
* Computationally efficient.
* Ability to handle mixed data types.

Here, an SVM classification model is tuned and trained to differentiate between Oyster and Cycle-hire database responses. This classifier is based on a selection of feature variables from the dataset prepared earlier in *'Introduction and Data Preparation'*. The optimally performing SVM classifier is then taken forward and tested by classifying survey data where *SurveyGp* database is known but unseen by the model.

The following general SVM classification procedure is followed [@kuhn:2013ab; @hsu:2016aa]:

1. Transform data to the format required by SVM (pre-processing).
2. Conduct simple scaling on the data.
3. Use an RBF kernel.
4. Use cross-validation to tune the model. 
5. Use the best model as a classifier to train the whole training set. 
6. Test performance on unseen but known data.

The tuning step involves a methodical search for values of parameters and hyperparameters that control SVM performance. Namely:

1. **Kernel type and bandwidth ($\sigma$)**: Kernels describe distance decay around predictor variables. A radial-basis (RBF) kernel ‘Gaussian’ generally performs well and will be used with a variety of bandwidth hyperparameters ($\sigma$).
2. **Error penalisation constant (C)**: A parameters controlling the level of acceptable error in the separation between classes. Large C values allow less training error but may overfit the model. Small C values may give larger training errors but result in a model that can generalise more easily.

The classification procedure is outlined below step-by-step and in greater detail.

All analyses take place in the `R` statistical platform (`R` version 3.3.0 (2016-05-03)) with fully documented code for transparency and to enable reproducible research. The following `R` packages were used:


```{r Load_libraries_data, include=TRUE, eval=TRUE, message=FALSE}
library(mlbench)
library(caret)
library(kernlab)
library(dplyr)
library(ggplot2)
library(doMC)
library(pROC)
library(knitr)
# Get from github if required
# devtools::install_github("sachsmc/plotROC")
library(plotROC)
```

*NB: This study contains all R code used to modify and analyse the data. Where appropriate this code will be shown alongside the explanatory text, however, for ease of reading, the majority of code will be hidden from view. All code, as well as explanatory text, can be found alongside one another in the raw .Rmd files accompanying this document.*

### Pre-processing

The *SurveyGp* classes were changed from integers to descriptive names in order to be accepted by the SVM training function later in the workflow.

```{r Rename_factors, include=TRUE, eval=FALSE}
# Rename SurveyGp factors - 2,3,4 not allowed as column names in caret::train
cdf.v <- cdf
cdf.v$SurveyGp <- as.character(cdf.v$SurveyGp)
cdf.v <- cdf.v %>%
  mutate(SurveyGp = ifelse(SurveyGp=="2", "Oyster", SurveyGp)) %>%
  mutate(SurveyGp = ifelse(SurveyGp=="3", "Cycle", SurveyGp)) %>%
  mutate(SurveyGp = ifelse(SurveyGp=="4", "Unknown", SurveyGp))
```


### Zero- and Near Zero-Variance Predictors

Some feature variables may possess values that occur at low frequencies. These 'near-zero-variance' (NZV) variables may have an undue influence on predictive models when they are randomly sampled as part of cross-validation. NZV variables are identified where the frequency ratio between the most common and second most common value of a variable is above 16 and the percent of unique values of a variable is less than 10 [@kuhn:2016aa].

The following predictors fulfilled these criteria and were removed from the dataset:

```{r NZV_find, include=TRUE, eval=TRUE, echo=FALSE}
# Find NZV predictors
# Remove ID and SurveyGp
nzv <- nearZeroVar(cdf.v[,-1:-2])
print(names(cdf.v[,nzv]))
```

```{r NVZ_remove, include=TRUE, eval=FALSE, echo=FALSE}
# Remove NZV predictors
cdf.temp <- cdf.v[,-1:-2]
cdf.nzv <- cdf.temp[-nzv]

# Add ID and SurveyGp back
cdf.nzv <- data.frame(c(cdf.v[,c("ID","SurveyGp")], cdf.nzv))

# Clean up
rm(cdf.temp, nzv)
```

### Partitioning data

The dataset was split into training and testing sets after eliminating the 'unknown' survey group (*SurveyGp*=4). An 80:20 training:testing ratio was applied<!-- cf. 60:20:20 recommendation of [@grolemund:2016aa] -->, stratified by class (ratio allocated equally within the two *SurveyGp* classes). The variable *ID* was removed from both training and testing sets.


```{r Partition, include=TRUE, eval=FALSE}
# Create unknown SurveyGp
cdf.unknown <- cdf.nzv %>%
  filter(SurveyGp=="Unknown")
cdf.unknown$SurveyGp <- factor(cdf.unknown$SurveyGp)

# Create known SurveyGp
cdf.known <- cdf.nzv %>%
  filter(SurveyGp!="Unknown")
cdf.known$SurveyGp <- factor(cdf.known$SurveyGp)
 
# Create training:test partition with 80:20 ratio stratified by class
set.seed(123)
train.index <- createDataPartition(cdf.known$SurveyGp, p = 0.8, list = FALSE)

# Create training set
training <- cdf.known[train.index, ]

# Create test set
testing <- cdf.known[-train.index,]

# Clean up
rm(cdf.known, train.index)
```


### SVM training and testing approach

<!-- Using: -->
<!-- * RBF kernel -->
<!-- * Estimate $\sigma$ using *kernlab::sigest* -->
<!-- * Repeated K–fold CV: creates multiple versions of the folds and aggregates the results (recommended by @kuhn:2013aa). -->

The training set was used to tune a classification model, and the testing set was used to confirm model performance on data unseen in model training. Classifiers may become 'over-fitted' to their training data and so perform poorly elsewhere. Applying the trained SVM classifier to the testing set provides an indication of how well the classifier generalises to new data.

Repeated 10-fold cross-validation was used to tune models by systematically searching a range of values of C and $\sigma$ and measuring model performance for each model. The procedure can be summarised as:

1. Randomly split the training data into 10 equal-sized folds (portions).
2. Leave one fold out and train models on the remaining 9 folds across a range of model parameters.
3. Test performance of these models on the left-out fold and save the results.
4. Repeat steps 1 to 3, successively leaving out a different fold each time.
5. Once all folds have been used, randomly split the training data again into 10-folds and repeat steps 1 to 4 -- do this five times.
6. Pick the best model using ROC (Receiver Operating Characteristic approach) and train a model on the whole training dataset.
7. Testing: apply parameters indicated in 6 to the testing set and evaluate classifier performance on unseen data.

### Model tuning

A grid-based search of C and $\sigma$ was used to tune the model. Nine values of C and $\sigma$ were searched: giving a total of 81 models trialled in total. Each model ran on the training dataset using 10-fold repeated cross-validation with 5 repeats. 

Scaling was performed within each cross-validation loop to 'obtain honest estimates of performance' [@kuhn:2013aa]. Feature variables need to be centred and scaled to zero mean and unit variance to prevent variables with smaller numeric ranges being dominated by those with larger ranges [@hsu:2016aa]. The same scaling factors used in training were also applied to testing datasets *ibid.*. 


```{r Allocate_cores, include=TRUE, eval=FALSE}
# For parallel processing - if your system allows - set the number of cores (workers)
registerDoMC(cores = 2)
```

```{r Train1_seeds, include=TRUE, eval=FALSE}
# Set seeds for reproducibility
## For reprodicibility, need to set seeds within resampling as running parallel processing.
## Number of seeds required: a list of B+1 elements (resamples plus 1) each containing vector of M integers where M is number of models being evaluated.
## In this case B = (5 repeats of 10-Fold CV) +1 = 51; M = nrow(svm.grid) (number of tuning combinations)
set.seed(123)
seeds1 <- vector(mode = "list", length = 51)
for(i in 1:50) seeds1[[i]] <- sample.int(1000, 81) ## number of models (combinations of parameters, ie. 9*9) evaluated
## For the last model:
seeds1[[51]] <- sample.int(1000, 1)
```


```{r Train1_control, include=TRUE, eval=FALSE}
# Specify training approach: Repeated 10–fold CV with 5 repeats
ctrl1 <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated five times
                           repeats = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           savePredictions = TRUE,
                           ## reproducability
                           seeds = seeds1)
```


```{r Train1_tune, include=TRUE, eval=FALSE}
#############################################################################
# NOT RUN - lengthy run using parallel processing with 2 cores:
# R version 3.3.0 (2016-05-03)
# Platform: x86_64-apple-darwin13.4.0 (64-bit)
# Running under: OS X 10.10.5 (Yosemite)

# Train model through parameter search using 10-fold repeated cross-validation
set.seed(1)
fit1 <- train(SurveyGp~., data = training[,-1], 
                 ## RBF kernel
                 method = "svmRadial",
                 ## parameters from caret::train
                 tuneLength = 9,
                 trControl = ctrl1,
                 preProcess = c("center", "scale"),
                 metric = "ROC",
                 verbose = FALSE)

# END NOT RUN
#############################################################################
```

```{r Save_training, include=TRUE, eval=FALSE, echo=FALSE}
save(list=ls(), file="./Data/pp.class_caret_v3.RData")
```


### Model selection

The optimal classifier was chosen by selecting values of C and $\sigma$ that resulted in the highest ROC (Receiver Operating Characteristic) AUC (Area Under Curve) value from the SVM classifier. The AUC represents the greatest probability that a training set survey response was assigned to its correct *SurveyGp*, and is derived from measuring the true positives and true negatives for each model. It is often used as a way to compare between binary classification models and to rate their performance [@james_introduction_2013].

Tuning showed that the best model produced an ROC of `r I(round(max(fit1$results[,"ROC"]), digits = 3))` used parameters of $\sigma=$ `r I(round(fit1$bestTune[[1]], digits = 3))` and C = `r I(fit1$bestTune[[2]])` (Figure 1). This model was not sparse - using `r I(fit1$finalModel@nSV)` of the 2095 observations as support vectors, possibly indicating that the model was overfitted. 

```{r Plot_training_params, echo=FALSE, fig.cap=paste("Figure 1: AUC values for a range of cost functions C for $\\sigma=$", round(fit1$bestTune[[1]], digits = 3), "in training.")}
ggplot(fit1)
```

The Confusion Matrix in the table below shows the share of true positives and false positives, and true and false negatives, where 0.5 was used as the cut-off between predictions in the model with highest AUC. There is a mean accuracy of 0.73 (73% correct assignment) in this model.

```{r Train1_CM, include=TRUE, eval=FALSE}
# Training confusion matrix using `caret::confusionMatrix`
fit1_train_pred <- predict(fit1, type = "raw")
fit1_train_CM <- confusionMatrix(fit1_train_pred, training$SurveyGp)
```

```{r Train1_CM_table, include=TRUE, echo=FALSE, eval=TRUE, echo=FALSE}
kable(fit1_train_CM$table,  caption = "**Table**: Confusion Matrix of the best training model applied to training set at 0.5 probability cut-off. Columns are Reference (Actual) values from the training set and rows are Predicted (Modelled) results from the best SVM classifier.")
```

### SVM testing

Parameters from the optimal SVM classifier were then used to re-train the model using the whole training dataset to generate a final model [@hsu:2016aa]. This final model was applied to the testing dataset to trial performance on 'unseen' data, and develop an appreciation of how such a model would perform on truly independent data (ie. the 'Unknown' *SurveyGp* data).


```{r Test1_CM, include=TRUE, eval=FALSE}
# Testing onfusion matrix using `caret::confusionMatrix`  
# Reference top, Prediction side
fit1_pred <- predict(fit1, newdata = testing[,-1:-2], type = "raw")
fit1_CM <- confusionMatrix(fit1_pred, testing$SurveyGp)
```

The Confusion Matrix for the testing set (below) shows an accuracy of 0.65 overall with a slightly greater Sensitivity to capturing correct assignment of Cycling than Oyster *SurveyGp*. The ROC curve in Figure 2 displays the overall performance of the optimal SVM classifier, with an AUC value of 0.70 (to 2 significant figures).

```{r Test1_CM_table, message=FALSE, results="asis" , echo=FALSE} 
kable(fit1_CM$table, caption = "**Table**: Confusion Matrix of the best training model applied to the testing set with propability cut-off of 0.5 between classes. Columns are Reference (Actual) values from the training set and rows are Predicted (Modelled) results from the best SVM classifier.")
```


```{r Test1_plotROC, echo=FALSE, warning=FALSE, fig.cap="Figure 2: ROC curve of optimal SVM classifier in classifying Oyster relative to Cycle-hire database groups using testing set.", fig.width=7, fig.height=7}
# Plot using ggplot extension plotROC
# Build dataframe with testing classes and porbabilities of testing fit from `caret::predict`
fit1_prob <- predict(fit1, newdata = testing[,-1:-2], type = "prob")
fit1_probex <- data.frame(obs = testing$SurveyGp, OysterProb = fit1_prob[, "Oyster"])

# Plot
fit1_ggROC <- ggplot(fit1_probex, aes(d = obs, m = OysterProb)) + 
  geom_roc(n.cuts = 8, labelround = 2, linealpha = 0.5)
styled_ggROC <- fit1_ggROC + 
  style_roc(theme = theme_grey, ylab = "True positives (Sensitivity)" , xlab = "True negatives (1 - Specificity)") +
  # ggtitle("ROC curve of Oyster and Cycle-hire database SVM test classification") +
  annotate("text", x = 0.75, y = 0.25, 
           label = paste("AUC =", round(calc_auc(fit1_ggROC)$AUC, 2)))

styled_ggROC

```

### Discussion

Perfect classifiers have AUC values of 1.0 - meaning that they give 100% true positives (perfect sensitivity) and 100% true negatives (perfect specificity). Such classifiers have ROC curves that hug the top left corner of the ROC plot. Completely ineffective classifiers that are no better than random possess an AUC of 0.5 - a 45$\unicode{xb0}$ diagonal line from bottom left to top right.

Testing of the optimal classifier developed for this study suggests that this model is capable overall of assigning survey records to their *SurveyGp* 70% of the time (AUC=0.70). Although accuracy of the classifier at the 50% classification threshold is 0.65 (95% confidence interval of 0.60-0.69). The shape of the ROC curve indicates that the sensitivity and specificity of the classifier is similar, meaning that discrimnation favours neither of the two *SurveyGp* classes. 

In the general literature an AUC performance of 0.7 would be considered poor to fair. However, the quality of the performance of a classifier can only be judged in the context of the problem being considered. The desirable AUC of a clinical test for a virile and deadly infectious disease may be set higher than that for a test for credit-worthiness during a bank account application for instance. In the case of the People and Places survey problem, is correct classification 70% of the time considered to be a sufficiently good level of performance to allow unknown database records to be classified and for them to be used in subsequent analyses? Following consultation with Rachel Aldred (Principal Investigator) the level of performance developed by this classification study was considered to be unacceptable. A decision was made not to proceed with classification of the 'unknown' database survey responses.

### Further work

Support Vector Machines are a generally successful supervised classification method, but SVMs are not the only method available. Neural Networks, Random Forests and Naive Bayesian Classification are alternative approaches that could be applied to the problem. In practice, however, the improvement in performance required to develop a classifier that would enable useful classification of 'unknown' database records - eg, capable of reaching AUC >0.9 - is likely beyond the reach of any other method [@kuhn:2013ab].

## References










